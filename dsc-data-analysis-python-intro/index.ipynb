{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis in Base Python - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lesson, you'll continue solidifying your knowledge of Python programming, descriptive statistics, and data visualization by performing key data analysis tasks. You will learn how to load data from CSV and JSON files into Python objects in memory, and wrap up with an analysis that joins together two datasets.\n",
    "\n",
    "## Base Python\n",
    "\n",
    "When we say \"base Python\", this means that we are emphasizing the data types and modules that are part of Python itself, rather than using third-party libraries. For much of this course, you will use numerous Python libraries such as `pandas` and StatsModels, but first we want to dig into how data analysis tasks can be performed without them.\n",
    "\n",
    "### Python Data Types and Control Structures\n",
    "\n",
    "In this lesson, our data is stored in data structures that are built into the Python language: numbers, strings, lists, dictionaries, etc. For example, if we have this table of data:\n",
    "\n",
    "| color  | number |\n",
    "| ------ | ------ |\n",
    "| green  | 7      |\n",
    "| red    | 2      |\n",
    "| orange | 1      |\n",
    "\n",
    "A standard way we might represent that table would be a list of dictionaries, where each dictionary represents a row and has the keys `color` and `number`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_table = [\n",
    "    {\"color\": \"green\", \"number\": 7},\n",
    "    {\"color\": \"red\", \"number\": 2},\n",
    "    {\"color\": \"orange\", \"number\": 1}\n",
    "]\n",
    "\n",
    "type(info_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_table = {\"color\":\"green\",\"number\":7}\n",
    "type (info_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if we wanted to print out all of the values associated with the `color` keys, the logic would look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "for key,value in info_table.items():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color\n",
      "number\n"
     ]
    }
   ],
   "source": [
    "for key, value in info_table.items():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   yield\n",
      "0     39.741234289561504\n",
      "1     39.872054738763474\n",
      "2      44.33116416558151\n",
      "3       46.6006230827385\n",
      "4     40.694984210927196\n",
      "...                  ...\n",
      "4995  39.037749535587466\n",
      "4996   51.86108583913893\n",
      "4997   36.44135204086599\n",
      "4998  42.549279557959224\n",
      "4999  34.798407319876624\n",
      "\n",
      "[5000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(\"data/apple_orchard.csv\") as f:\n",
    "    apple_orchard_data = pd.DataFrame(csv.DictReader(f))\n",
    "    print(apple_orchard_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##creat a function which takes two arguments (key, links).create a dictionary that will iterate through the links and extract the key to each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freq_table(key,links):\n",
    "    table ={}\n",
    "    for link in links:\n",
    "        user = link[key]\n",
    "        table[user] = table.get(user,0)+1\n",
    "    return table\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (603873998.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    * Central Tendency ( Mean, Median)\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "##Load the apple orchard csv file and perform the following analysis:\n",
    "* Count\n",
    "* Central Tendency ( Mean, Median)\n",
    "* Maximum\n",
    "* Minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As more knowledge is gained, that same task (printing the list of colors) might look something like this, using the `pandas` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(info_table)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mtype\u001b[39m(df)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:667\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[0;32m--> 667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_series:\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(info_table)\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(df[\"color\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the base Python example, we had a variable `info_table` which was type `list` (a built-in Python type), whereas in the pandas example, we had a variable `df` which was type `pandas.core.frame.DataFrame` (a custom type from the pandas library).\n",
    "\n",
    "Then to print out the colors, in the base Python example we had a `for` loop and then code to extract the information from each individual dictionary. The string values got printed out without any additional markup. Then in the pandas example we did not have to use a loop (hint: this is *broadcasting* like we saw previously with NumPy) and also we had some extra markup where it printed out the index values as well as `Name: color, dtype: object`.\n",
    "\n",
    "In this lesson you will practice performing data analysis with the first (base Python) format, before eventually learning how to use pandas instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Modules\n",
    "\n",
    "In addition to the data types used, we will also use built-in Python modules. These modules must be imported, but they are part of Python itself and do not require separate installation.\n",
    "\n",
    "The main modules we will focus on are `csv` ([documentation here](https://docs.python.org/3/library/csv.html)) and `json` ([documentation here](https://docs.python.org/3/library/json.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data from Files\n",
    "\n",
    "While trivial example data structures like the one above can be declared directly in Python code, more realistic data comes in the form of a file on disk.\n",
    "\n",
    "In this lesson you will practice opening data files that use the CSV and JSON formats, then loading them into sensible objects using the `csv` and `json` libraries in order to perform additional analysis.\n",
    "\n",
    "### CSV Files\n",
    "\n",
    "The first major file type we will explore is CSV (comma-separated value). For example, this code loads a dataset of 5000 apple trees in an orchard, where the column `yield` represents the total pounds of apples yielded by that tree for a given year.\n",
    "\n",
    "You can think of the table as looking like this, except that it is thousands of lines long:\n",
    "\n",
    "| yield              |\n",
    "| -----------------  |\n",
    "| 39.741234289561504 |\n",
    "| 39.872054738763474 |\n",
    "| 44.33116416558151  |\n",
    "| 46.6006230827385   |\n",
    "| 40.694984210927196 |\n",
    "| 40.96981882686812  |\n",
    "\n",
    "In relatively few lines of code, we can perform some key data analysis tasks: counting, measuring central tendency, finding the maximum, and finding the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total apple tree yields: 5000\n",
      "Average apple tree yield: 42.40762385776258\n",
      "Maximum apple tree yield: 65.54817042071103\n",
      "Minimum apple tree yield: 21.93121221285836\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/apple_orchard.csv\") as f:\n",
    "    apple_orchard_data = list(csv.DictReader(f))\n",
    "    apple_tree_yields = [float(x[\"yield\"]) for x in apple_orchard_data]\n",
    "    print(\"Total apple tree yields:\", len(apple_tree_yields))\n",
    "    print(\"Average apple tree yield:\", sum(apple_tree_yields) / len(apple_tree_yields))\n",
    "    print(\"Maximum apple tree yield:\", max(apple_tree_yields))\n",
    "    print(\"Minimum apple tree yield:\", min(apple_tree_yields))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Files\n",
    "\n",
    "The second major file type we will explore is JSON (JavaScript object notation). For example, this code loads a dataset of interactions between Twitter users, where each user is represented as a \"node\" and when one tweets at another that connection is represented as a \"link\".\n",
    "\n",
    "You can think of this data as looking like this, although this version is truncated:\n",
    "\n",
    "```\n",
    "{\n",
    "  'nodes': [\n",
    "    {'id': '347457291'},\n",
    "    {'id': '232762581'},\n",
    "    {'id': '23678636'},\n",
    "    {'id': '21278412'},\n",
    "    {'id': '222040026'},\n",
    "    {'id': '19579205'},\n",
    "    {'id': '222957350'},\n",
    "    {'id': '264013722'},\n",
    "    ...\n",
    "  ],\n",
    "  'links': [\n",
    "    {'source': '347457291', 'target': '232762581'},\n",
    "    {'source': '347457291', 'target': '119706266'},\n",
    "    {'source': '347457291', 'target': '421509544'},\n",
    "    {'source': '232762581', 'target': '222957350'},\n",
    "    {'source': '232762581', 'target': '21648607'},\n",
    "    {'source': '232762581', 'target': '155691033'},\n",
    "    {'source': '232762581', 'target': '59974294'},\n",
    "    ...\n",
    "  ]\n",
    "  ...\n",
    "}\n",
    "```\n",
    "\n",
    "In relatively few lines of code, we can count the number of users and find the users who were \"sources\" (the user initiating a tweet that tags someone else) most often and \"targets\" (the user being tagged in a tweet) most often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 99 users in this dataset\n",
      "\n",
      "Top 5 Sources:\n",
      "User 232762581\t| 23 Tweets\n",
      "User 49076695\t| 20 Tweets\n",
      "User 523173553\t| 19 Tweets\n",
      "User 24883888\t| 17 Tweets\n",
      "User 53318310\t| 16 Tweets\n",
      "\n",
      "Top 5 Targets:\n",
      "User 169686021\t| 13 Tweets\n",
      "User 23642374\t| 12 Tweets\n",
      "User 25797630\t| 11 Tweets\n",
      "User 25626212\t| 11 Tweets\n",
      "User 21648607\t| 10 Tweets\n"
     ]
    }
   ],
   "source": [
    "def build_freq_table(key, links):\n",
    "    table = {}\n",
    "    for link in links:\n",
    "        user = link[key]\n",
    "        table[user] = table.get(user, 0) + 1\n",
    "    return table\n",
    "\n",
    "def print_top_5(table):\n",
    "    for k, v in sorted(table.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"User {k}\\t| {v} Tweets\")\n",
    "\n",
    "with open(\"data/twitter_graph.json\") as f:\n",
    "    twitter_data = json.load(f)\n",
    "    print(f\"There are {len(twitter_data['nodes'])} users in this dataset\")\n",
    "    print()\n",
    "    \n",
    "    links = twitter_data[\"links\"]\n",
    "    sources = build_freq_table(\"source\", links)\n",
    "    targets = build_freq_table(\"target\", links)\n",
    "        \n",
    "    print(\"Top 5 Sources:\")\n",
    "    print_top_5(sources)\n",
    "    print()\n",
    "    print(\"Top 5 Targets:\")\n",
    "    print_top_5(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What About NumPy and Matplotlib?\n",
    "\n",
    "You can continue using NumPy and Matplotlib as you see fit. For example, if you want to convert a base Python list into a NumPy array because it will help you perform some descriptive analysis task, that is not an issue. You will also need to use Matplotlib in the lessons ahead to create the required visualizations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson you will learn how to load and manipulate CSV and JSON datasets using base Python. This means that we will mainly be using the data structures and modules built into Python, rather than third-party libraries. We also walked through some examples of the kinds of analysis you will be able to perform by the end of this lesson."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
